"""Batch class for mini-batch parallelization, adapted from PyTorch Geometric repository at https://github.com/rusty1s/pytorch_geometric, by Kirk Swanson"""
# Load modules
from __future__ import print_function, division
import torch
from Data_PyTorch import Data

class Batch(Data):
    """Batch class for mini-batch parallelization"""

    def __init__(self, batch=None, **kwargs):
        super(Batch, self).__init__(**kwargs)
        self.batch = batch

    @staticmethod
    def from_data_list(data_list):
        """Constructes an object of class Batch from a list of graphs generated by a DataLoader"""

        keys = data_list[0].keys
        assert 'batch' not in keys

        batch = Batch()

        for key in keys:
            batch[key] = []
        batch.batch = []

        cumsum = 0
        for i, data in enumerate(data_list):
            num_nodes = data.num_nodes
            batch.batch.append(torch.full((num_nodes, ), i, dtype=torch.long))
            for key in keys:
                item = data[key]
                item = item + cumsum if batch.cumsum(key, item) else item # Ensures that the cumulative edge_index matrix of the batch will have different sets of indices for each subgraph of the batch
                batch[key].append(item)
            cumsum += num_nodes

        for key in keys:
            batch[key] = torch.cat(batch[key], dim=data_list[0].cat_dim(key)) # Concatenates batch['edge_index'] differently than the others (and also possibly batch['y'])
        batch.batch = torch.cat(batch.batch, dim=-1)
        return batch.contiguous()

    def cumsum(self, key, item):
        """Essentially checks for presence of 'edge_index' attribute"""
        return item.dim() > 1 and item.dtype == torch.long

    @property
    def num_graphs(self):
        """Compute the number of graphs in the batch"""
        return self.batch[-1].item() + 1